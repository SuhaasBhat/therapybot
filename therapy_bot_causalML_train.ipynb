{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "from typing import Dict, List, Tuple\n",
    "from collections import Counter\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "from transformers import (\n",
    "    MODEL_WITH_LM_HEAD_MAPPING,\n",
    "    WEIGHTS_NAME,\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "MODEL_CONFIG_CLASSES = list(MODEL_WITH_LM_HEAD_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n",
    "\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except ImportError:\n",
    "    from tensorboardX import SummaryWriter\n",
    "\n",
    "# Configs\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "MODEL_CONFIG_CLASSES = list(MODEL_WITH_LM_HEAD_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "therapy_dataset = pd.read_csv(\"dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript_id</th>\n",
       "      <th>mi_quality</th>\n",
       "      <th>video_title</th>\n",
       "      <th>video_url</th>\n",
       "      <th>topic</th>\n",
       "      <th>utterance_id</th>\n",
       "      <th>interlocutor</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>utterance_text</th>\n",
       "      <th>main_therapist_behaviour</th>\n",
       "      <th>client_talk_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>high</td>\n",
       "      <td>NEW VIDEO: Brief intervention: \"Barbara\"</td>\n",
       "      <td>https://www.youtube.com/watch?v=PaSKcfTmFEk</td>\n",
       "      <td>reducing alcohol consumption</td>\n",
       "      <td>0</td>\n",
       "      <td>therapist</td>\n",
       "      <td>00:00:13</td>\n",
       "      <td>Thanks for filling it out. We give this form t...</td>\n",
       "      <td>question</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>high</td>\n",
       "      <td>NEW VIDEO: Brief intervention: \"Barbara\"</td>\n",
       "      <td>https://www.youtube.com/watch?v=PaSKcfTmFEk</td>\n",
       "      <td>reducing alcohol consumption</td>\n",
       "      <td>1</td>\n",
       "      <td>client</td>\n",
       "      <td>00:00:24</td>\n",
       "      <td>Sure.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>high</td>\n",
       "      <td>NEW VIDEO: Brief intervention: \"Barbara\"</td>\n",
       "      <td>https://www.youtube.com/watch?v=PaSKcfTmFEk</td>\n",
       "      <td>reducing alcohol consumption</td>\n",
       "      <td>2</td>\n",
       "      <td>therapist</td>\n",
       "      <td>00:00:25</td>\n",
       "      <td>So, let's see. It looks that you put-- You dri...</td>\n",
       "      <td>therapist_input</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>high</td>\n",
       "      <td>NEW VIDEO: Brief intervention: \"Barbara\"</td>\n",
       "      <td>https://www.youtube.com/watch?v=PaSKcfTmFEk</td>\n",
       "      <td>reducing alcohol consumption</td>\n",
       "      <td>3</td>\n",
       "      <td>client</td>\n",
       "      <td>00:00:34</td>\n",
       "      <td>Mm-hmm.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>high</td>\n",
       "      <td>NEW VIDEO: Brief intervention: \"Barbara\"</td>\n",
       "      <td>https://www.youtube.com/watch?v=PaSKcfTmFEk</td>\n",
       "      <td>reducing alcohol consumption</td>\n",
       "      <td>4</td>\n",
       "      <td>therapist</td>\n",
       "      <td>00:00:34</td>\n",
       "      <td>-and you usually have three to four drinks whe...</td>\n",
       "      <td>therapist_input</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9694</th>\n",
       "      <td>133</td>\n",
       "      <td>high</td>\n",
       "      <td>Motivational interviewing: Ken interviews Daryl</td>\n",
       "      <td>https://www.youtube.com/watch?v=MOVj0FoOxpk&amp;t=...</td>\n",
       "      <td>reducing recidivism</td>\n",
       "      <td>376</td>\n",
       "      <td>therapist</td>\n",
       "      <td>00:26:45</td>\n",
       "      <td>You know maybe they'll walk that road with you...</td>\n",
       "      <td>other</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9695</th>\n",
       "      <td>133</td>\n",
       "      <td>high</td>\n",
       "      <td>Motivational interviewing: Ken interviews Daryl</td>\n",
       "      <td>https://www.youtube.com/watch?v=MOVj0FoOxpk&amp;t=...</td>\n",
       "      <td>reducing recidivism</td>\n",
       "      <td>377</td>\n",
       "      <td>client</td>\n",
       "      <td>00:26:49</td>\n",
       "      <td>Yeah, okay, uh, all right, yeah. All right.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>change</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9696</th>\n",
       "      <td>133</td>\n",
       "      <td>high</td>\n",
       "      <td>Motivational interviewing: Ken interviews Daryl</td>\n",
       "      <td>https://www.youtube.com/watch?v=MOVj0FoOxpk&amp;t=...</td>\n",
       "      <td>reducing recidivism</td>\n",
       "      <td>378</td>\n",
       "      <td>therapist</td>\n",
       "      <td>00:26:52</td>\n",
       "      <td>There's a couple of people.</td>\n",
       "      <td>other</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9697</th>\n",
       "      <td>133</td>\n",
       "      <td>high</td>\n",
       "      <td>Motivational interviewing: Ken interviews Daryl</td>\n",
       "      <td>https://www.youtube.com/watch?v=MOVj0FoOxpk&amp;t=...</td>\n",
       "      <td>reducing recidivism</td>\n",
       "      <td>379</td>\n",
       "      <td>client</td>\n",
       "      <td>00:26:54</td>\n",
       "      <td>Yeah.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>change</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9698</th>\n",
       "      <td>133</td>\n",
       "      <td>high</td>\n",
       "      <td>Motivational interviewing: Ken interviews Daryl</td>\n",
       "      <td>https://www.youtube.com/watch?v=MOVj0FoOxpk&amp;t=...</td>\n",
       "      <td>reducing recidivism</td>\n",
       "      <td>380</td>\n",
       "      <td>therapist</td>\n",
       "      <td>00:26:58</td>\n",
       "      <td>If you want to do that. It gotta be your call.</td>\n",
       "      <td>therapist_input</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9699 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      transcript_id mi_quality  \\\n",
       "0                 0       high   \n",
       "1                 0       high   \n",
       "2                 0       high   \n",
       "3                 0       high   \n",
       "4                 0       high   \n",
       "...             ...        ...   \n",
       "9694            133       high   \n",
       "9695            133       high   \n",
       "9696            133       high   \n",
       "9697            133       high   \n",
       "9698            133       high   \n",
       "\n",
       "                                          video_title  \\\n",
       "0            NEW VIDEO: Brief intervention: \"Barbara\"   \n",
       "1            NEW VIDEO: Brief intervention: \"Barbara\"   \n",
       "2            NEW VIDEO: Brief intervention: \"Barbara\"   \n",
       "3            NEW VIDEO: Brief intervention: \"Barbara\"   \n",
       "4            NEW VIDEO: Brief intervention: \"Barbara\"   \n",
       "...                                               ...   \n",
       "9694  Motivational interviewing: Ken interviews Daryl   \n",
       "9695  Motivational interviewing: Ken interviews Daryl   \n",
       "9696  Motivational interviewing: Ken interviews Daryl   \n",
       "9697  Motivational interviewing: Ken interviews Daryl   \n",
       "9698  Motivational interviewing: Ken interviews Daryl   \n",
       "\n",
       "                                              video_url  \\\n",
       "0           https://www.youtube.com/watch?v=PaSKcfTmFEk   \n",
       "1           https://www.youtube.com/watch?v=PaSKcfTmFEk   \n",
       "2           https://www.youtube.com/watch?v=PaSKcfTmFEk   \n",
       "3           https://www.youtube.com/watch?v=PaSKcfTmFEk   \n",
       "4           https://www.youtube.com/watch?v=PaSKcfTmFEk   \n",
       "...                                                 ...   \n",
       "9694  https://www.youtube.com/watch?v=MOVj0FoOxpk&t=...   \n",
       "9695  https://www.youtube.com/watch?v=MOVj0FoOxpk&t=...   \n",
       "9696  https://www.youtube.com/watch?v=MOVj0FoOxpk&t=...   \n",
       "9697  https://www.youtube.com/watch?v=MOVj0FoOxpk&t=...   \n",
       "9698  https://www.youtube.com/watch?v=MOVj0FoOxpk&t=...   \n",
       "\n",
       "                             topic  utterance_id interlocutor timestamp  \\\n",
       "0     reducing alcohol consumption             0    therapist  00:00:13   \n",
       "1     reducing alcohol consumption             1       client  00:00:24   \n",
       "2     reducing alcohol consumption             2    therapist  00:00:25   \n",
       "3     reducing alcohol consumption             3       client  00:00:34   \n",
       "4     reducing alcohol consumption             4    therapist  00:00:34   \n",
       "...                            ...           ...          ...       ...   \n",
       "9694           reducing recidivism           376    therapist  00:26:45   \n",
       "9695           reducing recidivism           377       client  00:26:49   \n",
       "9696           reducing recidivism           378    therapist  00:26:52   \n",
       "9697           reducing recidivism           379       client  00:26:54   \n",
       "9698           reducing recidivism           380    therapist  00:26:58   \n",
       "\n",
       "                                         utterance_text  \\\n",
       "0     Thanks for filling it out. We give this form t...   \n",
       "1                                                 Sure.   \n",
       "2     So, let's see. It looks that you put-- You dri...   \n",
       "3                                               Mm-hmm.   \n",
       "4     -and you usually have three to four drinks whe...   \n",
       "...                                                 ...   \n",
       "9694  You know maybe they'll walk that road with you...   \n",
       "9695        Yeah, okay, uh, all right, yeah. All right.   \n",
       "9696                        There's a couple of people.   \n",
       "9697                                              Yeah.   \n",
       "9698     If you want to do that. It gotta be your call.   \n",
       "\n",
       "     main_therapist_behaviour client_talk_type  \n",
       "0                    question              NaN  \n",
       "1                         NaN          neutral  \n",
       "2             therapist_input              NaN  \n",
       "3                         NaN          neutral  \n",
       "4             therapist_input              NaN  \n",
       "...                       ...              ...  \n",
       "9694                    other              NaN  \n",
       "9695                      NaN           change  \n",
       "9696                    other              NaN  \n",
       "9697                      NaN           change  \n",
       "9698          therapist_input              NaN  \n",
       "\n",
       "[9699 rows x 11 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "therapy_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.Series.reset_index(therapy_dataset.loc[therapy_dataset['transcript_id']==23]['utterance_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi, Hannah. Nice to meet you.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['utterance_text'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexted_train = []\n",
    "contexted_val = []\n",
    "\n",
    "n = 7 #hyperparameter, determines length of the conversation - tradeoff with amount of data\n",
    "\n",
    "for conversation_id in range(0,133):\n",
    "    #only want to add therapist responses to the dataset, as that's all I care about\n",
    "    conv_series = pd.Series.reset_index(therapy_dataset.loc[therapy_dataset['transcript_id']==conversation_id]['utterance_text'])\n",
    "    for i in range(n, len(conv_series['utterance_text'])):\n",
    "        if i%2 == 0: #condition on currently\n",
    "            row = []\n",
    "            prev = i - 1 - n # we additionally substract 1, so row will contain current response and 7 previous responces  \n",
    "            for j in range(i, prev, -1):\n",
    "                #if j%2 == 0:  #controls for therapist? no, this means we will only even look at therapist dialogue. \n",
    "                    row.append(conv_series['utterance_text'][j])\n",
    "            if conversation_id > 120: #hyperparameter: train test split\n",
    "                contexted_val.append(row)\n",
    "            else:\n",
    "                contexted_train.append(row)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3685\n"
     ]
    }
   ],
   "source": [
    "print(len(contexted_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['response',\n",
       " 'context',\n",
       " 'context/0',\n",
       " 'context/1',\n",
       " 'context/2',\n",
       " 'context/3',\n",
       " 'context/4',\n",
       " 'context/5']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['response', 'context'] \n",
    "columns = columns + ['context/'+str(i) for i in range(n-1)]\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>context</th>\n",
       "      <th>context/0</th>\n",
       "      <th>context/1</th>\n",
       "      <th>context/2</th>\n",
       "      <th>context/3</th>\n",
       "      <th>context/4</th>\n",
       "      <th>context/5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>You kinda juggle those two things and kinda--</td>\n",
       "      <td>I mean the thing is when you're a-- I mean, I'...</td>\n",
       "      <td>Right.</td>\n",
       "      <td>Well, I used to be quite a heavy smoker, right...</td>\n",
       "      <td>-when you eat. That's one aspect. What other a...</td>\n",
       "      <td>Well—</td>\n",
       "      <td>So one thing that you noticed is diet and the ...</td>\n",
       "      <td>Yeah. I do miss her breakfast. I have to say, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>That's right.</td>\n",
       "      <td>Oh, I see. So-so it's like a-it's like a balance.</td>\n",
       "      <td>You need insulin in your cells, individual cel...</td>\n",
       "      <td>Are you saying that they-they like they can't ...</td>\n",
       "      <td>All right. And Type II diabetes typically peop...</td>\n",
       "      <td>Right.</td>\n",
       "      <td>So that's a hormone or a chemical, it allows y...</td>\n",
       "      <td>I've heard of it yeah.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>So it's been a bit of a-a bit of a shock, but ...</td>\n",
       "      <td>\"-sitting in the corner of the bar, and chat, ...</td>\n",
       "      <td>Carrying on.</td>\n",
       "      <td>-because I think to myself, \"Well, if someday,...</td>\n",
       "      <td>So rather than--</td>\n",
       "      <td>Uh, maybe this is a blessing, actually-</td>\n",
       "      <td>Indeed, yeah.</td>\n",
       "      <td>-which he says can be managed, and you seem to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>All right, and he's right. This medication wil...</td>\n",
       "      <td>He told me that I had a blood test for cholest...</td>\n",
       "      <td>All right. What did the doctor tell you choles...</td>\n",
       "      <td>Sure. I'm just getting a few groceries.</td>\n",
       "      <td>Hello, Kaylie. I'm Lori, the pharmacist here a...</td>\n",
       "      <td>Fantastic. All right. Well, here's your prescr...</td>\n",
       "      <td>Uh, I think you answered all my questions. Tha...</td>\n",
       "      <td>Great, Kaylie. I know we went over a lot today...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>The test, is the test, and experience, yeah.</td>\n",
       "      <td>Um, I can't argue with a medical professional,...</td>\n",
       "      <td>Mm.</td>\n",
       "      <td>I mean, it's you know, why?</td>\n",
       "      <td>Feel like you need to. Yeah.</td>\n",
       "      <td>No, I don't want to, because I don't feel that...</td>\n",
       "      <td>Yeah. And your thought was, yeah, right, I hea...</td>\n",
       "      <td>Yeah. It was um, and I-I said, well, no. I'm s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>Hmm. So, it doesn't make total sense to you th...</td>\n",
       "      <td>So, I don't know if it's that-that's where I'm...</td>\n",
       "      <td>Okay.</td>\n",
       "      <td>Well, we keep them clean, like he's cleaned. S...</td>\n",
       "      <td>-but I wonder what are your experiences with O...</td>\n",
       "      <td>Mm-hmm.</td>\n",
       "      <td>-what's next. Right. Okay. So, I mean, there i...</td>\n",
       "      <td>Mm-hmm, what's next, yeah, yeah.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>Thank you.</td>\n",
       "      <td>And I'm glad that this appointment was a posit...</td>\n",
       "      <td>Yeah.</td>\n",
       "      <td>Okay. I respect that. Thanks though for coming...</td>\n",
       "      <td>I'm not ready for that conversation right now.</td>\n",
       "      <td>Okay.</td>\n",
       "      <td>Yeah, maybe another time. Just, I'm not ready ...</td>\n",
       "      <td>Sure. I totally appreciate that, and it's your...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>I think you answered all my questions. Thank you.</td>\n",
       "      <td>That's great, Kaylie. I know we went over a lo...</td>\n",
       "      <td>Sure. I'm just gonna take one tablet of the ch...</td>\n",
       "      <td>Just so we know we're on the same page, can yo...</td>\n",
       "      <td>All right. Well, this medication is usually we...</td>\n",
       "      <td>He didn't mention anything about that.</td>\n",
       "      <td>What did your doctor tell you to expect in ter...</td>\n",
       "      <td>All right. So that sounds like a perfect place...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>Um, I don't really want to go there. I just-- ...</td>\n",
       "      <td>Would you be open to kind of exploring the ide...</td>\n",
       "      <td>Yeah, no.</td>\n",
       "      <td>Okay. That's a big step and that's not the dir...</td>\n",
       "      <td>I don't even know. I'm-I'm not ready to have a...</td>\n",
       "      <td>So what would have happened if you found out t...</td>\n",
       "      <td>It's really good news.</td>\n",
       "      <td>Okay.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>Sure.</td>\n",
       "      <td>But this is what I'm gonna do. All right? And ...</td>\n",
       "      <td>It's fine.</td>\n",
       "      <td>All right?</td>\n",
       "      <td>Yep.</td>\n",
       "      <td>This is between you and me.</td>\n",
       "      <td>Mhm.</td>\n",
       "      <td>-this and this.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              response  \\\n",
       "138      You kinda juggle those two things and kinda--   \n",
       "73                                       That's right.   \n",
       "229  So it's been a bit of a-a bit of a shock, but ...   \n",
       "357  All right, and he's right. This medication wil...   \n",
       "180       The test, is the test, and experience, yeah.   \n",
       "379  Hmm. So, it doesn't make total sense to you th...   \n",
       "418                                         Thank you.   \n",
       "365  I think you answered all my questions. Thank you.   \n",
       "409  Um, I don't really want to go there. I just-- ...   \n",
       "277                                              Sure.   \n",
       "\n",
       "                                               context  \\\n",
       "138  I mean the thing is when you're a-- I mean, I'...   \n",
       "73   Oh, I see. So-so it's like a-it's like a balance.   \n",
       "229  \"-sitting in the corner of the bar, and chat, ...   \n",
       "357  He told me that I had a blood test for cholest...   \n",
       "180  Um, I can't argue with a medical professional,...   \n",
       "379  So, I don't know if it's that-that's where I'm...   \n",
       "418  And I'm glad that this appointment was a posit...   \n",
       "365  That's great, Kaylie. I know we went over a lo...   \n",
       "409  Would you be open to kind of exploring the ide...   \n",
       "277  But this is what I'm gonna do. All right? And ...   \n",
       "\n",
       "                                             context/0  \\\n",
       "138                                             Right.   \n",
       "73   You need insulin in your cells, individual cel...   \n",
       "229                                       Carrying on.   \n",
       "357  All right. What did the doctor tell you choles...   \n",
       "180                                                Mm.   \n",
       "379                                              Okay.   \n",
       "418                                              Yeah.   \n",
       "365  Sure. I'm just gonna take one tablet of the ch...   \n",
       "409                                          Yeah, no.   \n",
       "277                                         It's fine.   \n",
       "\n",
       "                                             context/1  \\\n",
       "138  Well, I used to be quite a heavy smoker, right...   \n",
       "73   Are you saying that they-they like they can't ...   \n",
       "229  -because I think to myself, \"Well, if someday,...   \n",
       "357            Sure. I'm just getting a few groceries.   \n",
       "180                        I mean, it's you know, why?   \n",
       "379  Well, we keep them clean, like he's cleaned. S...   \n",
       "418  Okay. I respect that. Thanks though for coming...   \n",
       "365  Just so we know we're on the same page, can yo...   \n",
       "409  Okay. That's a big step and that's not the dir...   \n",
       "277                                         All right?   \n",
       "\n",
       "                                             context/2  \\\n",
       "138  -when you eat. That's one aspect. What other a...   \n",
       "73   All right. And Type II diabetes typically peop...   \n",
       "229                                   So rather than--   \n",
       "357  Hello, Kaylie. I'm Lori, the pharmacist here a...   \n",
       "180                       Feel like you need to. Yeah.   \n",
       "379  -but I wonder what are your experiences with O...   \n",
       "418     I'm not ready for that conversation right now.   \n",
       "365  All right. Well, this medication is usually we...   \n",
       "409  I don't even know. I'm-I'm not ready to have a...   \n",
       "277                                               Yep.   \n",
       "\n",
       "                                             context/3  \\\n",
       "138                                              Well—   \n",
       "73                                              Right.   \n",
       "229            Uh, maybe this is a blessing, actually-   \n",
       "357  Fantastic. All right. Well, here's your prescr...   \n",
       "180  No, I don't want to, because I don't feel that...   \n",
       "379                                            Mm-hmm.   \n",
       "418                                              Okay.   \n",
       "365             He didn't mention anything about that.   \n",
       "409  So what would have happened if you found out t...   \n",
       "277                        This is between you and me.   \n",
       "\n",
       "                                             context/4  \\\n",
       "138  So one thing that you noticed is diet and the ...   \n",
       "73   So that's a hormone or a chemical, it allows y...   \n",
       "229                                      Indeed, yeah.   \n",
       "357  Uh, I think you answered all my questions. Tha...   \n",
       "180  Yeah. And your thought was, yeah, right, I hea...   \n",
       "379  -what's next. Right. Okay. So, I mean, there i...   \n",
       "418  Yeah, maybe another time. Just, I'm not ready ...   \n",
       "365  What did your doctor tell you to expect in ter...   \n",
       "409                             It's really good news.   \n",
       "277                                               Mhm.   \n",
       "\n",
       "                                             context/5  \n",
       "138  Yeah. I do miss her breakfast. I have to say, ...  \n",
       "73                              I've heard of it yeah.  \n",
       "229  -which he says can be managed, and you seem to...  \n",
       "357  Great, Kaylie. I know we went over a lot today...  \n",
       "180  Yeah. It was um, and I-I said, well, no. I'm s...  \n",
       "379                   Mm-hmm, what's next, yeah, yeah.  \n",
       "418  Sure. I totally appreciate that, and it's your...  \n",
       "365  All right. So that sounds like a perfect place...  \n",
       "409                                              Okay.  \n",
       "277                                    -this and this.  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_df = pd.DataFrame.from_records(contexted_train, columns=columns)\n",
    "trn_df = trn_df.sample(frac=1) #shuffle\n",
    "val_df = pd.DataFrame.from_records(contexted_val, columns=columns)\n",
    "val_df = val_df.sample(frac=1)\n",
    "val_df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##our data is complete! onto training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    def __init__(self):\n",
    "        self.output_dir = 'output_large_1'\n",
    "        self.model_type = 'gpt2'\n",
    "        self.model_name_or_path = 'microsoft/DialoGPT-medium' #on Supercloud, this has to be local\n",
    "        self.config_name = 'microsoft/DialoGPT-medium'\n",
    "        self.tokenizer_name = 'microsoft/DialoGPT-medium' \n",
    "        self.cache_dir = 'cached'\n",
    "        self.block_size = 512\n",
    "        self.do_train = True\n",
    "        self.do_eval = True\n",
    "        self.evaluate_during_training = False \n",
    "        self.per_gpu_train_batch_size = 4\n",
    "        self.per_gpu_eval_batch_size = 1\n",
    "        self.gradient_accumulation_steps = 1\n",
    "        self.learning_rate = 5e-5 \n",
    "        self.weight_decay = 0.0\n",
    "        self.adam_epsilon = 1e-8\n",
    "        self.max_grad_norm = 1.0\n",
    "        self.num_train_epochs = 6\n",
    "        self.max_steps = -1\n",
    "        self.warmup_steps = 0\n",
    "        self.logging_steps = 2000\n",
    "        self.save_steps = 2000\n",
    "        self.save_total_limit = None\n",
    "        self.eval_all_checkpoints = True\n",
    "        self.no_cuda = False ####CAREFUL\n",
    "        self.overwrite_output_dir = True\n",
    "        self.overwrite_cache = True\n",
    "        self.should_continue = False\n",
    "        self.seed = 42\n",
    "        self.local_rank = -1\n",
    "        self.fp16 = False\n",
    "        self.fp16_opt_level = 'O1'\n",
    "\n",
    "args = Args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, cache_dir=args.cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_conv(row, tokenizer, eos = True):\n",
    "    # from: https://stackoverflow.com/questions/952914/how-to-make-a-flat-list-out-of-list-of-lists\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    conv = list(reversed([tokenizer.encode(x) + [tokenizer.eos_token_id] for x in row]))\n",
    "    conv = flatten(conv)\n",
    "    return conv\n",
    "\n",
    "class ConversationDataset(Dataset):\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, args, df, block_size=512):\n",
    "\n",
    "        block_size = block_size - (tokenizer.model_max_length - tokenizer.max_len_single_sentence)\n",
    "\n",
    "        directory = args.cache_dir\n",
    "        cached_features_file = os.path.join(\n",
    "            directory, args.model_type + \"_cached_lm_\" + str(block_size)\n",
    "        )\n",
    "\n",
    "        if os.path.exists(cached_features_file) and not args.overwrite_cache:\n",
    "            logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
    "            with open(cached_features_file, \"rb\") as handle:\n",
    "                self.examples = pickle.load(handle)\n",
    "        else:\n",
    "            logger.info(\"Creating features from dataset file at %s\", directory)\n",
    "\n",
    "            self.examples = []\n",
    "            for _, row in df.iterrows():\n",
    "                conv = construct_conv(row, tokenizer)\n",
    "                if len(conv) > block_size: continue\n",
    "                self.examples.append(conv)\n",
    "\n",
    "            # Note that we are loosing the last truncated example here for the sake of simplicity (no padding)\n",
    "            # If your dataset is small, first you should loook for a bigger one :-) and second you\n",
    "            # can change this behavior by adding (model specific) padding.\n",
    "\n",
    "            logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
    "            with open(cached_features_file, \"wb\") as handle:\n",
    "                pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return torch.tensor(self.examples[item], dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cacheing and storing of data/checkpoints\n",
    "\n",
    "def load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False):\n",
    "    return ConversationDataset(tokenizer, args, df_val if evaluate else df_trn)\n",
    "\n",
    "\n",
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "\n",
    "def _sorted_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> List[str]:\n",
    "    ordering_and_checkpoint_path = []\n",
    "\n",
    "    glob_checkpoints = glob.glob(os.path.join(args.output_dir, \"{}-*\".format(checkpoint_prefix)))\n",
    "\n",
    "    for path in glob_checkpoints:\n",
    "        if use_mtime:\n",
    "            ordering_and_checkpoint_path.append((os.path.getmtime(path), path))\n",
    "        else:\n",
    "            regex_match = re.match(\".*{}-([0-9]+)\".format(checkpoint_prefix), path)\n",
    "            if regex_match and regex_match.groups():\n",
    "                ordering_and_checkpoint_path.append((int(regex_match.groups()[0]), path))\n",
    "\n",
    "    checkpoints_sorted = sorted(ordering_and_checkpoint_path)\n",
    "    checkpoints_sorted = [checkpoint[1] for checkpoint in checkpoints_sorted]\n",
    "    return checkpoints_sorted\n",
    "\n",
    "\n",
    "def _rotate_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> None:\n",
    "    if not args.save_total_limit:\n",
    "        return\n",
    "    if args.save_total_limit <= 0:\n",
    "        return\n",
    "\n",
    "    # Check if we should delete older checkpoint(s)\n",
    "    checkpoints_sorted = _sorted_checkpoints(args, checkpoint_prefix, use_mtime)\n",
    "    if len(checkpoints_sorted) <= args.save_total_limit:\n",
    "        return\n",
    "\n",
    "    number_of_checkpoints_to_delete = max(0, len(checkpoints_sorted) - args.save_total_limit)\n",
    "    checkpoints_to_be_deleted = checkpoints_sorted[:number_of_checkpoints_to_delete]\n",
    "    for checkpoint in checkpoints_to_be_deleted:\n",
    "        logger.info(\"Deleting older checkpoint [{}] due to args.save_total_limit\".format(checkpoint))\n",
    "        shutil.rmtree(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training of model\n",
    "\n",
    "def train(args, train_dataset, model: PreTrainedModel, tokenizer: PreTrainedTokenizer) -> Tuple[int, float]:\n",
    "    \"\"\" Train the model \"\"\"\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        tb_writer = SummaryWriter()\n",
    "\n",
    "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
    "\n",
    "    def collate(examples: List[torch.Tensor]):\n",
    "        if tokenizer._pad_token is None:\n",
    "            return pad_sequence(examples, batch_first=True)\n",
    "        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, collate_fn=collate, drop_last = True\n",
    "    )\n",
    "\n",
    "    if args.max_steps > 0:\n",
    "        t_total = args.max_steps\n",
    "        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
    "    else:\n",
    "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "\n",
    "    model = model.module if hasattr(model, \"module\") else model  # Take care of distributed/parallel training\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    # add_special_tokens_(model, tokenizer)\n",
    "\n",
    "\n",
    "    # Prepare optimizer and schedule (linear warmup and decay)\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": args.weight_decay,\n",
    "        },\n",
    "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
    "    )\n",
    "\n",
    "    # Check if saved optimizer or scheduler states exist\n",
    "    if (\n",
    "        args.model_name_or_path\n",
    "        and os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\"))\n",
    "        and os.path.isfile(os.path.join(args.model_name_or_path, \"scheduler.pt\"))\n",
    "    ):\n",
    "        # Load in optimizer and scheduler states\n",
    "        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n",
    "        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n",
    "\n",
    "    if args.fp16:\n",
    "        try:\n",
    "            from apex import amp\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
    "\n",
    "    # multi-gpu training (should be after apex fp16 initialization)\n",
    "    if args.n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Distributed training (should be after apex fp16 initialization)\n",
    "    if args.local_rank != -1:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(\n",
    "            model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True\n",
    "        )\n",
    "\n",
    "    # Train!\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
    "    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
    "    logger.info(\n",
    "        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
    "        args.train_batch_size\n",
    "        * args.gradient_accumulation_steps\n",
    "        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n",
    "    )\n",
    "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
    "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "\n",
    "    global_step = 0\n",
    "    epochs_trained = 0\n",
    "    steps_trained_in_current_epoch = 0\n",
    "    # Check if continuing training from a checkpoint\n",
    "    if args.model_name_or_path and os.path.exists(args.model_name_or_path):\n",
    "        try:\n",
    "            # set global_step to gobal_step of last saved checkpoint from model path\n",
    "            checkpoint_suffix = args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0]\n",
    "            global_step = int(checkpoint_suffix)\n",
    "            epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n",
    "            steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n",
    "\n",
    "            logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n",
    "            logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n",
    "            logger.info(\"  Continuing training from global step %d\", global_step)\n",
    "            logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n",
    "        except ValueError:\n",
    "            logger.info(\"  Starting fine-tuning.\")\n",
    "\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "\n",
    "    model.zero_grad()\n",
    "    train_iterator = trange(\n",
    "        epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n",
    "    )\n",
    "    set_seed(args)  # Added here for reproducibility\n",
    "    for _ in train_iterator:\n",
    "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "\n",
    "            # Skip past any already trained steps if resuming training\n",
    "            if steps_trained_in_current_epoch > 0:\n",
    "                steps_trained_in_current_epoch -= 1\n",
    "                continue\n",
    "\n",
    "            inputs, labels = (batch, batch)\n",
    "            if inputs.shape[1] > 1024: continue\n",
    "            inputs = inputs.to(args.device)\n",
    "            labels = labels.to(args.device)\n",
    "            model.train()\n",
    "            outputs = model(inputs, labels=labels)\n",
    "            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n",
    "\n",
    "            if args.n_gpu > 1:\n",
    "                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "\n",
    "            if args.fp16:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                if args.fp16:\n",
    "                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
    "                else:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "                optimizer.step()\n",
    "                scheduler.step()  # Update learning rate schedule\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
    "                    # Log metrics\n",
    "                    if (\n",
    "                        args.local_rank == -1 and args.evaluate_during_training\n",
    "                    ):  # Only evaluate when single GPU otherwise metrics may not average well\n",
    "                        results = evaluate(args, model, tokenizer)\n",
    "                        for key, value in results.items():\n",
    "                            tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n",
    "                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n",
    "                    tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n",
    "                    logging_loss = tr_loss\n",
    "\n",
    "                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n",
    "                    checkpoint_prefix = \"checkpoint\"\n",
    "                    # Save model checkpoint\n",
    "                    output_dir = os.path.join(args.output_dir, \"{}-{}\".format(checkpoint_prefix, global_step))\n",
    "                    os.makedirs(output_dir, exist_ok=True)\n",
    "                    model_to_save = (\n",
    "                        model.module if hasattr(model, \"module\") else model\n",
    "                    )  # Take care of distributed/parallel training\n",
    "                    model_to_save.save_pretrained(output_dir)\n",
    "                    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n",
    "                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "\n",
    "                    _rotate_checkpoints(args, checkpoint_prefix)\n",
    "\n",
    "                    torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
    "                    torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
    "                    logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n",
    "\n",
    "            if args.max_steps > 0 and global_step > args.max_steps:\n",
    "                epoch_iterator.close()\n",
    "                break\n",
    "        if args.max_steps > 0 and global_step > args.max_steps:\n",
    "            train_iterator.close()\n",
    "            break\n",
    "\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        tb_writer.close()\n",
    "\n",
    "    return global_step, tr_loss / global_step\n",
    "\n",
    "# Evaluation of some model\n",
    "\n",
    "def evaluate(args, model: PreTrainedModel, tokenizer: PreTrainedTokenizer, df_trn, df_val, prefix=\"\") -> Dict:\n",
    "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
    "    eval_output_dir = args.output_dir\n",
    "\n",
    "    eval_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=True)\n",
    "    os.makedirs(eval_output_dir, exist_ok=True)\n",
    "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
    "    # Note that DistributedSampler samples randomly\n",
    "\n",
    "    def collate(examples: List[torch.Tensor]):\n",
    "        if tokenizer._pad_token is None:\n",
    "            return pad_sequence(examples, batch_first=True)\n",
    "        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "    eval_sampler = SequentialSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(\n",
    "        eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size, collate_fn=collate, drop_last = True\n",
    "    )\n",
    "\n",
    "    # multi-gpu evaluate\n",
    "    if args.n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Eval!\n",
    "    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
    "    logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
    "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    model.eval()\n",
    "\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        inputs, labels = (batch, batch)\n",
    "        inputs = inputs.to(args.device)\n",
    "        labels = labels.to(args.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs, labels=labels)\n",
    "            lm_loss = outputs[0]\n",
    "            eval_loss += lm_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    perplexity = torch.exp(torch.tensor(eval_loss))\n",
    "\n",
    "    result = {\"perplexity\": perplexity}\n",
    "\n",
    "    output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n",
    "    with open(output_eval_file, \"w\") as writer:\n",
    "        logger.info(\"***** Eval results {} *****\".format(prefix))\n",
    "        for key in sorted(result.keys()):\n",
    "            logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(df_trn, df_val):\n",
    "    args = Args()\n",
    "    \n",
    "    if args.should_continue:\n",
    "        sorted_checkpoints = _sorted_checkpoints(args)\n",
    "        if len(sorted_checkpoints) == 0:\n",
    "            raise ValueError(\"Used --should_continue but no checkpoint was found in --output_dir.\")\n",
    "        else:\n",
    "            args.model_name_or_path = sorted_checkpoints[-1]\n",
    "\n",
    "    if (\n",
    "        os.path.exists(args.output_dir)\n",
    "        and os.listdir(args.output_dir)\n",
    "        and args.do_train\n",
    "        and not args.overwrite_output_dir\n",
    "        and not args.should_continue\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n",
    "                args.output_dir\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Setup CUDA, GPU & distributed training\n",
    "    \n",
    "    #ADD AVAILABILITY DEPENDENCY:\n",
    "    device = torch.device(\"cpu\")\n",
    "    if torch.cuda.is_available:\n",
    "        device = torch.device(\"cuda\") \n",
    "    args.n_gpu = torch.cuda.device_count()\n",
    "    args.device = device\n",
    "\n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n",
    "    )\n",
    "    logger.warning(\n",
    "        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "        args.local_rank,\n",
    "        device,\n",
    "        args.n_gpu,\n",
    "        bool(args.local_rank != -1),\n",
    "        args.fp16,\n",
    "    )\n",
    "\n",
    "    # Set seed\n",
    "    set_seed(args)\n",
    "\n",
    "    config = AutoConfig.from_pretrained(args.config_name, cache_dir=args.cache_dir)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, cache_dir=args.cache_dir)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        args.model_name_or_path,\n",
    "        from_tf=False,\n",
    "        config=config,\n",
    "        cache_dir=args.cache_dir,\n",
    "    )\n",
    "    model.to(args.device)\n",
    "    \n",
    "    logger.info(\"Training/evaluation parameters %s\", args)\n",
    "\n",
    "    # Training\n",
    "    if args.do_train:\n",
    "        train_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False)\n",
    "\n",
    "        global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n",
    "        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
    "\n",
    "    # Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()\n",
    "    if args.do_train:\n",
    "        # Create output directory if needed\n",
    "        os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n",
    "        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "        # They can then be reloaded using `from_pretrained()`\n",
    "        model_to_save = (\n",
    "            model.module if hasattr(model, \"module\") else model\n",
    "        )  # Take care of distributed/parallel training\n",
    "        model_to_save.save_pretrained(args.output_dir)\n",
    "        tokenizer.save_pretrained(args.output_dir)\n",
    "\n",
    "        # Good practice: save your training arguments together with the trained model\n",
    "        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n",
    "\n",
    "        # Load a trained model and vocabulary that you have fine-tuned\n",
    "        model = AutoModelForCausalLM.from_pretrained(args.output_dir)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n",
    "        model.to(args.device)\n",
    "\n",
    "    # Evaluation\n",
    "    results = {}\n",
    "    if args.do_eval and args.local_rank in [-1, 0]:\n",
    "        checkpoints = [args.output_dir]\n",
    "        if args.eval_all_checkpoints:\n",
    "            checkpoints = list(\n",
    "                os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True))\n",
    "            )\n",
    "            logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n",
    "        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
    "        for checkpoint in checkpoints:\n",
    "            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n",
    "            prefix = checkpoint.split(\"/\")[-1] if checkpoint.find(\"checkpoint\") != -1 else \"\"\n",
    "\n",
    "            model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "            model.to(args.device)\n",
    "            result = evaluate(args, model, tokenizer, df_trn, df_val, prefix=prefix)\n",
    "            result = dict((k + \"_{}\".format(global_step), v) for k, v in result.items())\n",
    "            results.update(result)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/08/2022 19:16:53 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "05/08/2022 19:17:02 - INFO - __main__ -   Training/evaluation parameters <__main__.Args object at 0x7fb0fed11930>\n",
      "05/08/2022 19:17:02 - INFO - __main__ -   Creating features from dataset file at cached\n",
      "05/08/2022 19:17:05 - INFO - __main__ -   Saving features into cached file cached/gpt2_cached_lm_512\n",
      "/home/gridsan/sbhat/.conda/envs/therapy/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "05/08/2022 19:17:05 - INFO - __main__ -   ***** Running training *****\n",
      "05/08/2022 19:17:05 - INFO - __main__ -     Num examples = 3658\n",
      "05/08/2022 19:17:05 - INFO - __main__ -     Num Epochs = 6\n",
      "05/08/2022 19:17:05 - INFO - __main__ -     Instantaneous batch size per GPU = 4\n",
      "05/08/2022 19:17:05 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "05/08/2022 19:17:05 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "05/08/2022 19:17:05 - INFO - __main__ -     Total optimization steps = 5484\n",
      "05/08/2022 19:17:05 - INFO - __main__ -     Starting fine-tuning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2b24b45e22a4ca6af27e871531cd881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0a9a3d77eb2430ebd15ac522ea37195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/914 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05844ae77ccd4d0e845378682b1803a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/914 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30bb44ca10094cf39d14b0af04c557d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/914 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gridsan/sbhat/.conda/envs/therapy/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "05/08/2022 19:27:31 - INFO - __main__ -   Saving model checkpoint to output_large_1/checkpoint-2000\n",
      "05/08/2022 19:27:35 - INFO - __main__ -   Saving optimizer and scheduler states to output_large_1/checkpoint-2000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2844442b19d1430ea4971c58445400c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/914 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04723521a5f44887bc85694cf047741d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/914 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/08/2022 19:38:10 - INFO - __main__ -   Saving model checkpoint to output_large_1/checkpoint-4000\n",
      "05/08/2022 19:38:14 - INFO - __main__ -   Saving optimizer and scheduler states to output_large_1/checkpoint-4000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eda102c939848cb85d7f471078ded21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/914 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/08/2022 19:45:59 - INFO - __main__ -    global_step = 5484, average loss = 0.9346102922122292\n",
      "05/08/2022 19:45:59 - INFO - __main__ -   Saving model checkpoint to output_large_1\n",
      "05/08/2022 19:46:09 - INFO - __main__ -   Evaluate the following checkpoints: ['output_large_1/checkpoint-2000', 'output_large_1/checkpoint-4000', 'output_large_1']\n",
      "05/08/2022 19:46:16 - INFO - __main__ -   Creating features from dataset file at cached\n",
      "05/08/2022 19:46:17 - INFO - __main__ -   Saving features into cached file cached/gpt2_cached_lm_512\n",
      "05/08/2022 19:46:17 - INFO - __main__ -   ***** Running evaluation checkpoint-2000 *****\n",
      "05/08/2022 19:46:17 - INFO - __main__ -     Num examples = 482\n",
      "05/08/2022 19:46:17 - INFO - __main__ -     Batch size = 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1286fab473f049a7bfd322432ec650b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/482 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/08/2022 19:46:30 - INFO - __main__ -   ***** Eval results checkpoint-2000 *****\n",
      "05/08/2022 19:46:30 - INFO - __main__ -     perplexity = tensor(31.4160)\n",
      "05/08/2022 19:46:36 - INFO - __main__ -   Creating features from dataset file at cached\n",
      "05/08/2022 19:46:37 - INFO - __main__ -   Saving features into cached file cached/gpt2_cached_lm_512\n",
      "05/08/2022 19:46:37 - INFO - __main__ -   ***** Running evaluation checkpoint-4000 *****\n",
      "05/08/2022 19:46:37 - INFO - __main__ -     Num examples = 482\n",
      "05/08/2022 19:46:37 - INFO - __main__ -     Batch size = 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2b59473a61d4d368b0dc973ef0792f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/482 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/08/2022 19:46:48 - INFO - __main__ -   ***** Eval results checkpoint-4000 *****\n",
      "05/08/2022 19:46:48 - INFO - __main__ -     perplexity = tensor(71.2219)\n",
      "05/08/2022 19:46:52 - INFO - __main__ -   Creating features from dataset file at cached\n",
      "05/08/2022 19:46:53 - INFO - __main__ -   Saving features into cached file cached/gpt2_cached_lm_512\n",
      "05/08/2022 19:46:53 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "05/08/2022 19:46:53 - INFO - __main__ -     Num examples = 482\n",
      "05/08/2022 19:46:53 - INFO - __main__ -     Batch size = 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "337c8d792dfd41a099a7a5abe441ad3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/482 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/08/2022 19:47:03 - INFO - __main__ -   ***** Eval results  *****\n",
      "05/08/2022 19:47:03 - INFO - __main__ -     perplexity = tensor(82.5036)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'perplexity_2000': tensor(31.4160),\n",
       " 'perplexity_4000': tensor(71.2219),\n",
       " 'perplexity_output_large_1': tensor(82.5036)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main(trn_df, val_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-therapy]",
   "language": "python",
   "name": "conda-env-.conda-therapy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
